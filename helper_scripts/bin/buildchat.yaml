compare: |
  ğŸ’» You are a helpful AI assistant.

  ğŸ‘¤
  Previously, you have a script like this:
  ```python
  {{ prev_code }}
  ```

  It's according running stdout is:
  ```
  {{ prev_stdout }}
  ```

  After improving the solution, you have a script like this:
  ```python
  {{ curr_code }}
  ```

  It's according running stdout is:
  ```
  {{ curr_stdout }}
  ```

  The prediction performance and related information are demonstrated in the stdout.
  I want to understand how the prediction performance are greatly improved.
  Please pay attention to the following points:
  - Is the prediction target changed?
  - Is the metric changed?

web:  | 
  ğŸ’» You are a helpful AI assistant.

  ğŸ‘¤ 
  You have a webpage with following content:
  ````markdown
  {{ markdown }}
  ````

  I want to discuss with you about the content of the webpage.
  Please response in Chinese.

pdf: |-
  ğŸ’» You are a helpful AI assistant.

  ğŸ‘¤ 
  You have a PDF with the following extracted text:
  ````text
  {{ text }}
  ````
  I want to discuss with you about the content of the webpage.
  Please response in Chinese.

pdf-option:
  paper: &paper-reading |-
    è¯·æ€»ç»“è¿™ç¯‡æ–‡ç« ï¼Œä¸»è¦å†…å®¹åŒ…å«ä»¥ä¸‹éƒ¨åˆ†
    1. å¦‚æœåªèšç„¦åœ¨ **æ–¹æ³•è®¾è®¡** æœ¬èº«çš„äº®ç‚¹ï¼ˆä¸è®¨è®ºè·¨é¢†åŸŸæˆæœä¸åº”ç”¨è¡¨ç°ï¼‰ï¼Œå½’çº³æ–‡ç« äº®ç‚¹
    2. éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ƒé€‰å–äº†ä»€ä¹ˆåœºæ™¯ç”¨äºè¯„æµ‹ï¼Œæ¯ä¸ªåœºæ™¯è¯•ç”¨çš„æŒ‡æ ‡æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿè¯·ç”¨è¡¨æ ¼å½¢å¼å±•ç¤º
    3. å’Œå“ªäº›Baselineåšäº†æ¯”è¾ƒï¼Œåœ¨æ–¹æ³•è®¾è®¡ä¸Šï¼ˆè€Œä¸æ˜¯å®éªŒç»“æœï¼‰ï¼Œç°åœ¨çš„æ–¹æ³•æ¯”baselineæ›´å¥½ï¼Ÿ

    æ¯ä¸ªå…³é”®ç‚¹åéƒ½æ¥ä¸€ä¸ª[<åŸæ–‡å†…å®¹>]çš„å¼•ç”¨(å¦‚æœæœ‰å›¾è¡¨ä¹Ÿè¦å¼•ç”¨)ï¼Œæ–¹ä¾¿æˆ‘Ctrl+FæŸ¥æ‰¾ã€‚

paper-option:
  reading: *paper-reading
  review: |-
    ä½ æ˜¯ä¸€ä¸ªNIPS/ICML/ICLR é¡¶å°–ä¼šè®®çš„reviewer, å¯¹äºè¿™ç¯‡paperï¼Œä½ ä¼šè§‰å¾—æœ‰ä»€ä¹ˆç¼ºé™·ï¼Ÿ
    å¯¹äºä½ æŒ‡æ˜çš„æ¯ä¸€å¤„ç¼ºé™·ï¼Œè¯·åˆ—å‡ºåŸæ–‡å¼•ç”¨ï¼Œæ–¹ä¾¿åç»­å»åŸæ–‡ä¸­æŸ¥æ‰¾&è¯å®ã€‚
  review1: |-
    ä½ æ˜¯ä¸€ä¸ªNIPS/ICML/ICLR é¡¶å°–ä¼šè®®çš„reviewer, å¯¹äºè¿™ç¯‡paperï¼Œ
    æ˜¯å¦æœ‰ä¸è‡ªæ´½çš„åœ°æ–¹ï¼Œæ¯”å¦‚æœ‰çš„ç‚¹é€»è¾‘é“¾è·¯ä¸å®Œæ•´ (å®Œæ•´çš„é€»è¾‘é“¾è·¯åº”è¯¥æ˜¯ æå‡ºé—®é¢˜-> æå‡ºè§£å†³æ–¹æ¡ˆ -> å®éªŒéªŒè¯)
    åŒæ—¶å¼•ç”¨åŸæ–‡å†…å®¹ï¼Œæ–¹ä¾¿åç»­å»åŸæ–‡ä¸­æŸ¥æ‰¾&è¯å®ã€‚
  review2: |-
    ä½ æ˜¯ä¸€ä¸ªNIPS/ICML/ICLR é¡¶å°–ä¼šè®®çš„reviewer, å¯¹äºè¿™ç¯‡paperï¼Œ
    è®ºæ–‡ä¸­æ˜¯å¦æœ‰è¡¨è¾¾ä¸æ¸…çš„é—®é¢˜ï¼Œä¾‹å¦‚
    - æŸäº›å¯èƒ½å¼•èµ·æ­§ä¹‰çš„æœ¯è¯­åœ¨é¦–æ¬¡å‡ºç°æ—¶æ²¡æœ‰è§£é‡Šæˆ–ç»™å‡ºå‚è€ƒæ–‡çŒ®
    - æŸäº›ç”¨è¯åœ¨å‰åå‡ºç°æ—¶ä¸ä¸€è‡´ï¼Œä»è€Œé€ æˆç†è§£å›°éš¾


explain_ws: |-
  ğŸ’» You are a helpful AI assistant.

  ğŸ‘¤ 
  You have a script like this:
  ```python
  {{ code }}
  ```

  It's according running stdout is:
  ```
  {{ stdout }}
  ```

  I want to understand how the prediction performance and labels are measured when creating the "score.csv".
  Please give a clear and precise explanation using mathematical expressions.
