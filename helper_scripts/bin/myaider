#!/bin/sh

# Check if CHAT_MODEL is set in the environment
# if [ -z "$CHAT_MODEL" ]; then
#   echo "Error: CHAT_MODEL environment variable is not set." >&2
#   exit 1
# fi

# Check if WEAK_MODEL is set in the environment, default to CHAT_MODEL if not
# if [ -z "$WEAK_MODEL" ]; then
#   WEAK_MODEL="$CHAT_MODEL"
# fi

# aider --model "$CHAT_MODEL" --weak-model "$WEAK_MODEL" --editor-model "$CHAT_MODEL" --architect --no-show-model-warnings --editor "nvim --cmd 'let g:flatten_wait=1' --cmd 'cnoremap wq lua vim.cmd(\"w\"); require\"snacks\".bufdelete()'" --watch-files --subtree-only --no-auto-commit "$@"
# I found architect mode will make it more accurate.

# the sota model;
# --reasoning-effort high
export LITELLM_PROXY_API_KEY=sk-1234
export LITELLM_PROXY_API_BASE=http://ep14.213428.xyz:4000
# litellm_proxy will make the streaming work.
aider --model "litellm_proxy/o3_coreai" --weak-model "litellm_proxy/gpt-4.1" --editor-model "litellm_proxy/gpt-4.1" --architect --no-show-model-warnings --editor "nvim --cmd 'let g:flatten_wait=1' --cmd 'cnoremap wq lua vim.cmd(\"w\"); require\"snacks\".bufdelete()'" --watch-files --subtree-only --no-auto-commit "$@"
# - o3 does not support streaming mode
